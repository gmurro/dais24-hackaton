{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41c0a10a-f1e9-47b3-9dae-1bb0a03b0d75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DressMe - A Simple RAG over luxury fashon products database\n",
    "\n",
    "DressMe use [Databricks Vector Search](https://docs.databricks.com/en/generative-ai/vector-search.html) integrating a [Databricks Foundation Model API](https://docs.databricks.com/en/machine-learning/foundation-models/index.html) embedding model (`llama-3-70b-instruct`).\n",
    "\n",
    "Retrieval-augmented generation (RAG) is one of the most popular application architectures for creating natural-language interfaces for people to interact with an organization's data. This notebook builds a very simple RAG application, with the following steps:\n",
    "\n",
    "1. Set up a vector index and configure it to automatically use an embedding model from the FMAPI to generate embeddings.\n",
    "2. Load some fashon products data into the vector database\n",
    "3. Query the database\n",
    "4. Build a prompt for an LLM from the query results\n",
    "5. Query an LLM via the FMAPI, using that prompt\n",
    "\n",
    "### Contributors ✨\n",
    "Team 4:\n",
    "- Giuseppe Murro - [gmurro](https://github.com/gmurro) - `giuseppe.murro@giorgioarmani.it`\n",
    "- Gianluca Sarà - [gians14ga](https://github.com/gians14ga) - `gianluca.sara@giorgioarmani.it`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1102d54c-03f9-4501-9e69-3c51d94a9ff1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "First, we will install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43421aa6-1401-47d1-ac80-1d255e8db479",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting databricks-vectorsearch\n",
      "  Using cached databricks_vectorsearch-0.38-py3-none-any.whl (13 kB)\n",
      "Collecting databricks-genai-inference\n",
      "  Using cached databricks_genai_inference-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting deprecation>=2\n",
      "  Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting protobuf<5,>=3.12.0\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Collecting requests>=2\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting mlflow-skinny<3,>=2.11.3\n",
      "  Using cached mlflow_skinny-2.13.2-py3-none-any.whl (5.3 MB)\n",
      "Collecting databricks-sdk==0.19.1\n",
      "  Using cached databricks_sdk-0.19.1-py3-none-any.whl (447 kB)\n",
      "Collecting pydantic>=2.4.2\n",
      "  Using cached pydantic-2.7.3-py3-none-any.whl (409 kB)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Collecting typing-extensions>=4.7.1\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting tenacity==8.2.3\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting pyyaml>=5.4.1\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Collecting google-auth~=2.0\n",
      "  Using cached google_auth-2.30.0-py2.py3-none-any.whl (193 kB)\n",
      "Collecting packaging\n",
      "  Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Collecting idna\n",
      "  Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Collecting certifi\n",
      "  Using cached certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Collecting anyio\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting click<9,>=7.0\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting cloudpickle<4\n",
      "  Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.0.0\n",
      "  Using cached opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
      "Collecting entrypoints<1\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting gitpython<4,>=3.1.9\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Collecting pytz<2025\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Collecting cachetools<6,>=5.0.0\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Using cached sqlparse-0.5.0-py3-none-any.whl (43 kB)\n",
      "Collecting importlib-metadata!=4.7.0,<8,>=3.7.0\n",
      "  Using cached importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting opentelemetry-api<3,>=1.0.0\n",
      "  Using cached opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.18.4\n",
      "  Using cached pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.46b0\n",
      "  Using cached opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
      "Collecting exceptiongroup>=1.0.2\n",
      "  Using cached exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Using cached wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Installing collected packages: pytz, zipp, wrapt, urllib3, typing-extensions, tenacity, sqlparse, sniffio, smmap, pyyaml, pyasn1, protobuf, packaging, idna, h11, exceptiongroup, entrypoints, cloudpickle, click, charset-normalizer, certifi, cachetools, annotated-types, rsa, requests, pydantic-core, pyasn1-modules, importlib-metadata, httpcore, gitdb, deprecation, deprecated, anyio, pydantic, opentelemetry-api, httpx, google-auth, gitpython, opentelemetry-semantic-conventions, databricks-sdk, opentelemetry-sdk, databricks-genai-inference, mlflow-skinny, databricks-vectorsearch\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2024.1\n",
      "    Uninstalling pytz-2024.1:\n",
      "      Successfully uninstalled pytz-2024.1\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.19.2\n",
      "    Uninstalling zipp-3.19.2:\n",
      "      Successfully uninstalled zipp-3.19.2\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.3\n",
      "    Uninstalling tenacity-8.2.3:\n",
      "      Successfully uninstalled tenacity-8.2.3\n",
      "  Attempting uninstall: sqlparse\n",
      "    Found existing installation: sqlparse 0.5.0\n",
      "    Uninstalling sqlparse-0.5.0:\n",
      "      Successfully uninstalled sqlparse-0.5.0\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.1\n",
      "    Uninstalling sniffio-1.3.1:\n",
      "      Successfully uninstalled sniffio-1.3.1\n",
      "  Attempting uninstall: smmap\n",
      "    Found existing installation: smmap 5.0.1\n",
      "    Uninstalling smmap-5.0.1:\n",
      "      Successfully uninstalled smmap-5.0.1\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.6.0\n",
      "    Uninstalling pyasn1-0.6.0:\n",
      "      Successfully uninstalled pyasn1-0.6.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.7\n",
      "    Uninstalling idna-3.7:\n",
      "      Successfully uninstalled idna-3.7\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: exceptiongroup\n",
      "    Found existing installation: exceptiongroup 1.2.1\n",
      "    Uninstalling exceptiongroup-1.2.1:\n",
      "      Successfully uninstalled exceptiongroup-1.2.1\n",
      "  Attempting uninstall: entrypoints\n",
      "    Found existing installation: entrypoints 0.4\n",
      "    Uninstalling entrypoints-0.4:\n",
      "      Successfully uninstalled entrypoints-0.4\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 3.0.0\n",
      "    Uninstalling cloudpickle-3.0.0:\n",
      "      Successfully uninstalled cloudpickle-3.0.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.7\n",
      "    Uninstalling click-8.1.7:\n",
      "      Successfully uninstalled click-8.1.7\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.6.2\n",
      "    Uninstalling certifi-2024.6.2:\n",
      "      Successfully uninstalled certifi-2024.6.2\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.3.3\n",
      "    Uninstalling cachetools-5.3.3:\n",
      "      Successfully uninstalled cachetools-5.3.3\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.7.0\n",
      "    Uninstalling annotated-types-0.7.0:\n",
      "      Successfully uninstalled annotated-types-0.7.0\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.9\n",
      "    Uninstalling rsa-4.9:\n",
      "      Successfully uninstalled rsa-4.9\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.18.4\n",
      "    Uninstalling pydantic_core-2.18.4:\n",
      "      Successfully uninstalled pydantic_core-2.18.4\n",
      "  Attempting uninstall: pyasn1-modules\n",
      "    Found existing installation: pyasn1_modules 0.4.0\n",
      "    Uninstalling pyasn1_modules-0.4.0:\n",
      "      Successfully uninstalled pyasn1_modules-0.4.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 7.1.0\n",
      "    Uninstalling importlib_metadata-7.1.0:\n",
      "      Successfully uninstalled importlib_metadata-7.1.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.5\n",
      "    Uninstalling httpcore-1.0.5:\n",
      "      Successfully uninstalled httpcore-1.0.5\n",
      "  Attempting uninstall: gitdb\n",
      "    Found existing installation: gitdb 4.0.11\n",
      "    Uninstalling gitdb-4.0.11:\n",
      "      Successfully uninstalled gitdb-4.0.11\n",
      "  Attempting uninstall: deprecation\n",
      "    Found existing installation: deprecation 2.1.0\n",
      "    Uninstalling deprecation-2.1.0:\n",
      "      Successfully uninstalled deprecation-2.1.0\n",
      "  Attempting uninstall: deprecated\n",
      "    Found existing installation: Deprecated 1.2.14\n",
      "    Uninstalling Deprecated-1.2.14:\n",
      "      Successfully uninstalled Deprecated-1.2.14\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.4.0\n",
      "    Uninstalling anyio-4.4.0:\n",
      "      Successfully uninstalled anyio-4.4.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.7.3\n",
      "    Uninstalling pydantic-2.7.3:\n",
      "      Successfully uninstalled pydantic-2.7.3\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.25.0\n",
      "    Uninstalling opentelemetry-api-1.25.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.25.0\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.0\n",
      "    Uninstalling httpx-0.27.0:\n",
      "      Successfully uninstalled httpx-0.27.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.30.0\n",
      "    Uninstalling google-auth-2.30.0:\n",
      "      Successfully uninstalled google-auth-2.30.0\n",
      "  Attempting uninstall: gitpython\n",
      "    Found existing installation: GitPython 3.1.43\n",
      "    Uninstalling GitPython-3.1.43:\n",
      "      Successfully uninstalled GitPython-3.1.43\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.46b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.46b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.46b0\n",
      "  Attempting uninstall: databricks-sdk\n",
      "    Found existing installation: databricks-sdk 0.19.1\n",
      "    Uninstalling databricks-sdk-0.19.1:\n",
      "      Successfully uninstalled databricks-sdk-0.19.1\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.25.0\n",
      "    Uninstalling opentelemetry-sdk-1.25.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.25.0\n",
      "  Attempting uninstall: databricks-genai-inference\n",
      "    Found existing installation: databricks-genai-inference 0.2.3\n",
      "    Uninstalling databricks-genai-inference-0.2.3:\n",
      "      Successfully uninstalled databricks-genai-inference-0.2.3\n",
      "  Attempting uninstall: mlflow-skinny\n",
      "    Found existing installation: mlflow-skinny 2.13.2\n",
      "    Uninstalling mlflow-skinny-2.13.2:\n",
      "      Successfully uninstalled mlflow-skinny-2.13.2\n",
      "  Attempting uninstall: databricks-vectorsearch\n",
      "    Found existing installation: databricks-vectorsearch 0.38\n",
      "    Uninstalling databricks-vectorsearch-0.38:\n",
      "      Successfully uninstalled databricks-vectorsearch-0.38\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-server 1.23.4 requires anyio<4,>=3.1.0, but you have anyio 4.4.0 which is incompatible.\n",
      "botocore 1.27.96 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.1 which is incompatible.\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.4.0 cachetools-5.3.3 certifi-2024.6.2 charset-normalizer-3.3.2 click-8.1.7 cloudpickle-3.0.0 databricks-genai-inference-0.2.3 databricks-sdk-0.19.1 databricks-vectorsearch-0.38 deprecated-1.2.14 deprecation-2.1.0 entrypoints-0.4 exceptiongroup-1.2.1 gitdb-4.0.11 gitpython-3.1.43 google-auth-2.30.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 importlib-metadata-7.1.0 mlflow-skinny-2.13.2 opentelemetry-api-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 packaging-24.1 protobuf-4.25.3 pyasn1-0.6.0 pyasn1-modules-0.4.0 pydantic-2.7.3 pydantic-core-2.18.4 pytz-2024.1 pyyaml-6.0.1 requests-2.32.3 rsa-4.9 smmap-5.0.1 sniffio-1.3.1 sqlparse-0.5.0 tenacity-8.2.3 typing-extensions-4.12.2 urllib3-2.2.1 wrapt-1.16.0 zipp-3.19.2\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall databricks-vectorsearch databricks-genai-inference\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "567a035b-23ec-4f82-ac33-b6058bedecf7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define catalog, table, endpoint, and index names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de74cb6-f62b-4fe6-aae9-2a0b9b19481f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"workspace\"\n",
    "DB='fashion'\n",
    "SOURCE_TABLE_NAME = \"fendi_products\"\n",
    "SOURCE_TABLE_FULLNAME=f\"{CATALOG}.{DB}.{SOURCE_TABLE_NAME}\"\n",
    "ORIGINAL_TABLE_FULLNAME=f\"bright_data_fashion_listings.datasets.fendi_products_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d95cd38e-fac3-41a3-979d-ff329879e84c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data preprocessing\n",
    "The `fendi_products_dataset` tables has been imported from the `Fashion Listings` datet in the Databricks Marketplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac13610-d143-4f0e-85f4-c1c48551ee7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|PRODUCT_NAME           |PRODUCT_DESCRIPTION                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|FF Belt                |Reversible belt with two loops. Iconic FF stud buckle. Made of black Cuoio Romano leather. PU lining with FF motif in gray and black. Black enameled metalware. Made in Italy                                                                                                                                                                                                                                             |\n",
      "|Shirt                  |Boxy cut cropped shirt. The long sleeves with side buttons can be worn rolled up and fastened with the ties. Made of white cotton poplin. Branded with patch pockets on the front in different shapes and sizes and embossed FF Baguette detail. Made in Italy                                                                                                                                                            |\n",
      "|Fendi Domino           |Compact, light design Fendi Domino sneakers. Made of fabric with jacquard FF motif in brown and tobacco. Rubber cupsole with embossed FF motif on the toe. Made in Italy                                                                                                                                                                                                                                                  |\n",
      "|Fendi Diagonal Belt Bag|Belt bag made of beige leather and fabric with FF motif. Embellished with dark brown leather diagonal insert. Internal compartment with zipper and palladium-finish metalware. Comes with an adjustable belt to be worn around the waist, on the shoulder or cross-body. Made in Italy                                                                                                                                    |\n",
      "|Cabin Size Trolley     |Four-wheeled suitcase made from fabric with black FF motif with double-slider zip fastening. Roomy inside compartment with adjustable nylon partition and mesh zip pocket. Palladium-finish metalware. Handle and telescopic handle, address tag and black leather details. Made in Italy                                                                                                                                 |\n",
      "|O’Lock driving loafers |Driving loafers. Flexible pebble soles. Made of black leather with FF fabric details. Embellished with a metal Fendi O’Lock buckle with an ultra-black ruthenium finish. Made in Italy                                                                                                                                                                                                                                    |\n",
      "|FF Belt                |Reversible belt with two loops. Iconic FF stud buckle. Made of black Cuoio Romano leather. Reverse side in fabric with tobacco and black FF motif. Black enameled metalware. Made in Italy                                                                                                                                                                                                                                |\n",
      "|Mon Tresor             |Small Mon Tresor bucket bag in a fabric with a jacquard FF motif with all-over shiny transparent sequin embroidery. Lined and with gold-finish metalware. Featuring two detachable black leather shoulder straps, one long and one short, to wear the bag over the shoulder or cross-body. Made in Italy                                                                                                                  |\n",
      "|Fendi First Midi       |Fendi First bag in the perfect size for daytime, made of soft, caramel-color nappa leather, with oversized metal F clasp bound in tone on tone nappa leather. Featuring an interior compartment lined in fabric with the iconic FF motif, removable inner hooks and gold-finish metalware. Can be carried by hand as a clutch or worn on the shoulder or cross-body thanks to the detachable shoulder strap. Made in Italy|\n",
      "|FF Eclissi Trunk Small |Small rectangular trunk made in fabric with the jacquard FF Eclissi motif, hand-shaded at the edges. Black leather details. Double snap clasp and padlock, interior compartment with newspaper pocket, and ultra-black metalwork with a ruthenium finish. Can be carried by hand or over the shoulder thanks to the single handle with key case tag and the adjustable, detachable shoulder strap. Made in Italy          |\n",
      "+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show few rows from fendi_products_dataset\n",
    "spark.sql(f\"\"\"SELECT PRODUCT_NAME, PRODUCT_DESCRIPTION\n",
    "          FROM {ORIGINAL_TABLE_FULLNAME}\n",
    "          WHERE COUNTRY=\"United States\"\n",
    "          LIMIT 10\n",
    "          \"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74fa939c-ef24-4ae3-8c38-4373e255c88b",
     "showTitle": true,
     "title": "Pyspark Schema Volume Table Setup"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the table in the current workspace and preprcess it\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{DB}\")\n",
    "spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {SOURCE_TABLE_FULLNAME} \n",
    "        USING delta \n",
    "        AS \n",
    "        SELECT *, CAST(PRICE as double) as price_double\n",
    "        FROM {ORIGINAL_TABLE_FULLNAME}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "          ALTER TABLE {SOURCE_TABLE_FULLNAME} SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.enableChangeDataFeed' = 'true')\n",
    "\"\"\")\n",
    "spark.sql(f\"\"\"\n",
    "          ALTER TABLE {SOURCE_TABLE_FULLNAME} DROP COLUMN PRICE\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5211a135-8aa4-4d2c-a98e-cb15836904d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Prepare context text column for index vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba40cf28-4b4c-47d4-aa7b-4654d712723e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new column CONTEXT_TEXT\n",
    "context_column = \"CONTEXT_TEXT\"\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {SOURCE_TABLE_FULLNAME}\n",
    "    ADD COLUMNS ({context_column} STRING)\n",
    "\"\"\")\n",
    "\n",
    "# Update the new column with concatenated values\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {SOURCE_TABLE_FULLNAME}\n",
    "    SET {context_column} = CONCAT('product_name=`', PRODUCT_NAME, '`; product_description=`', PRODUCT_DESCRIPTION, '`; product_color=`', COLOR, '`')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca886cd4-c328-4687-bf46-9c4237b4b629",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CONTEXT_TEXT                                                                                                                                                                                                                                                                                                                     |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|product_name=`Fendi Cloud Slides`; product_description=`Wide band Fendi Cloud slides. Made of black rubber with the embossed FF motifs. Made in Italy`; product_color=`Black`                                                                                                                                                    |\n",
      "|product_name=`Fendi Diagonal Wallet`; product_description=`Wallet with two internal compartments for bills and outer pocket with press-stud fastening. Made of textured fabric with gray and black FF motif. Embellished with a black and yellow leather inlay. Made in Italy`; product_color=`Black`                            |\n",
      "|product_name=`Coat`; product_description=`Long, straight-cut, single-breasted coat. Lapel collar and breast pocket. Centre slit at the back. Two-button closure. Lined. Made of dark blue double-sided cashmere. Finished with double-layer effect matching details on the cuffs and collar. Made in Italy`; product_color=`Blue`|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show a sample of created column\n",
    "spark.sql(f\"\"\"SELECT {context_column}\n",
    "          FROM {SOURCE_TABLE_FULLNAME}\n",
    "          WHERE COUNTRY=\"United States\"\n",
    "          LIMIT 3\n",
    "          \"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4e7db85-65c7-4f1a-b23f-a55203275800",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Set up the Vector Database\n",
    "Next, we set up the vector database. There are three key steps:\n",
    "1. Initialize the vector search client\n",
    "2. Create the endpoint\n",
    "3. Create the index using the source Delta table we created earlier and the `bge-large-en` embeddings model from the Foundation Model API\n",
    "\n",
    "### Initialize the Vector Search Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "638fb80d-9774-418a-8073-53ea831b6b5b",
     "showTitle": true,
     "title": "Vector Search Client Initialization"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n"
     ]
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c73400ec-2e4e-4f5b-b372-1401a43a3f71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create the Endpoint\n",
    "\n",
    "The cell below will check if the endpoint already exists and create it if it does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e05dcf9-7678-4622-9ef4-d3e1029fbc61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint fashion_endpoint already exists.\n",
      "Endpoint fashion_endpoint is ONLINE.\n"
     ]
    }
   ],
   "source": [
    "VS_ENDPOINT_NAME = 'fashion_endpoint'\n",
    "\n",
    "if vsc.list_endpoints().get('endpoints') == None or not VS_ENDPOINT_NAME in [endpoint.get('name') for endpoint in vsc.list_endpoints().get('endpoints')]:\n",
    "    print(f\"Creating new Vector Search endpoint named {VS_ENDPOINT_NAME}\")\n",
    "    vsc.create_endpoint(VS_ENDPOINT_NAME)\n",
    "else:\n",
    "    print(f\"Endpoint {VS_ENDPOINT_NAME} already exists.\")\n",
    "\n",
    "vsc.wait_for_endpoint(VS_ENDPOINT_NAME, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "869939f1-c211-4973-a3be-c99d72beea98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create the Vector Index\n",
    "\n",
    "Now we can create the index over the Delta table we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14ce5a7a-bd7a-43af-9af5-7793f7f81f08",
     "showTitle": true,
     "title": "Python Delta Sync Index Setup"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vector Index...\n"
     ]
    }
   ],
   "source": [
    "VS_INDEX_NAME = 'fashion_assistant_vs_index'\n",
    "VS_INDEX_FULLNAME = f\"{CATALOG}.{DB}.{VS_INDEX_NAME}\"\n",
    "\n",
    "if not VS_INDEX_FULLNAME in [index.get(\"name\") for index in vsc.list_indexes(VS_ENDPOINT_NAME).get('vector_indexes', [])]:\n",
    "    try:\n",
    "        # set up an index with managed embeddings\n",
    "        print(\"Creating Vector Index...\")\n",
    "        i = vsc.create_delta_sync_index_and_wait(\n",
    "            endpoint_name=VS_ENDPOINT_NAME,\n",
    "            index_name=VS_INDEX_FULLNAME,\n",
    "            source_table_name=SOURCE_TABLE_FULLNAME,\n",
    "            pipeline_type=\"TRIGGERED\",\n",
    "            primary_key=\"PRODUCT_ID\",\n",
    "            embedding_source_column=context_column,\n",
    "            embedding_model_endpoint_name=\"databricks-bge-large-en\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if \"INTERNAL_ERROR\" in str(e):\n",
    "            # Check if the index exists after the error occurred\n",
    "            if VS_INDEX_FULLNAME in [index.get(\"name\") for index in vsc.list_indexes(VS_ENDPOINT_NAME).get('vector_indexes', [])]:\n",
    "                print(f\"Index {VS_INDEX_FULLNAME} has been created.\")\n",
    "            else:\n",
    "                raise e\n",
    "        else:\n",
    "            raise e\n",
    "else:\n",
    "    print(f\"Index {VS_INDEX_FULLNAME} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3173bf07-4917-410c-a581-fde7fab801e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We specified `embedding_model_endpoint_name=\"databricks-bge-large-en\"`. By passing an `embedding_source_column` and `embedding_model_endpoint_name`, we configure the index such that it will automatically use the model to generate embeddings for the texts in the `text` column of the source table. We do not need to manually generate embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "517ed55b-11f0-4ca0-8cb3-26a6713c31bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Sync the Vector Search Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80887325-5542-4387-a5be-c537e537ca1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sync\n",
    "index = vsc.get_index(endpoint_name=VS_ENDPOINT_NAME,\n",
    "                      index_name=VS_INDEX_FULLNAME)\n",
    "index.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da431c89-0d73-4769-ae35-4fd900528263",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Chat with DressMe \n",
    "\n",
    "Chat with the LLM and get featured answer based on the most related products picked from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cfb944-ce14-4edd-a161-4b1224af8506",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User question: I don't know what to wear next week for my new job. I'm a man an i like to wear casual clothes. Can you provide me an appelling outfit to buy?\n",
      "\n",
      "DressMe answer: Congratulations on your new job!\n",
      "\n",
      "I've got just the outfit for you - a stylish, yet casual look that's perfect for your new role. Here's a suggested ensemble:\n",
      "\n",
      "**Top:** Start with the **Fendi Black Cotton Jersey T-Shirt** (https://static.fendi.com/dam/is/image/fendi/FAF532AD3CF0GME_01?wid=768&hei=768&hash=f72f74aa5c3dfa592d2b0e68e622e75a-17e1bcde15b&sw=768&sh=768). Its regular fit and crew neck will provide a comfortable and classic look.\n",
      "\n",
      "**Bottom:** Pair the tee with the **Fendi Brown Silk Twill Trousers** (https://static.fendi.com/dam/is/image/fendi/FR5994A8G3F118W_01?wid=960&hei=960&hash=3ac464177b8ff56b6bb9e7b41662ff22-17e1d4a9395&sw=960&sh=960). The flowing wide-leg design and all-over FF motif print will add a touch of sophistication to your outfit.\n",
      "\n",
      "**Shoes:** Complete the look with the **Fendi Match Lace-Up Sneakers** (https://static.fendi.com/dam/is/image/fendi/8E8358AHH2F1FHS_01?wid=1000&hei=1000&hash=61c4f0dd3a32e2b893a9a0651bc8b5ba-182d330d968). The white leather and pale grey suede details will add a sleek, modern touch to your overall look.\n",
      "\n",
      "This outfit is both stylish and comfortable, perfect for your new job. You'll look confident and put-together without sacrificing your casual style!\n"
     ]
    }
   ],
   "source": [
    "from databricks_genai_inference import ChatSession\n",
    "\n",
    "# reset history\n",
    "chat = ChatSession(model=\"databricks-meta-llama-3-70b-instruct\",\n",
    "                   system_message=\"You are a helpful fashion assistant. Answer the user's question based on the provided context.\",\n",
    "                   max_tokens=512)\n",
    "\n",
    "user_question = \"I don't know what to wear next week for my new job. I'm a man an i like to wear casual clothes. Can you provide me an appelling outfit to buy?\"\n",
    "\n",
    "# get context from vector search\n",
    "raw_context_top = index.similarity_search(columns=[ \"PRODUCT_NAME\", \"PRODUCT_DESCRIPTION\", \"IMAGE\"],\n",
    "                        query_text=f\"{user_question} Based on the question, provide the most related top wearing (shirt, pullover, t-shirt etc) that fits the requirements.\",\n",
    "                        num_results = 1)\n",
    "\n",
    "raw_context_bottom = index.similarity_search(columns=[ \"PRODUCT_NAME\", \"PRODUCT_DESCRIPTION\", \"IMAGE\"],\n",
    "                        query_text=\"{user_question} Based on the question, provide a bottom wearing (pants, shorts, jeans etc) that fits the requirements.\",\n",
    "                        num_results = 1)\n",
    "\n",
    "raw_context_shoes = index.similarity_search(columns=[ \"PRODUCT_NAME\", \"PRODUCT_DESCRIPTION\", \"IMAGE\"],\n",
    "                        query_text=\"Advice a pair of shoes.\",\n",
    "                        num_results = 1)\n",
    "\n",
    "context_string = \"Context:\\n\\n\"\n",
    "\n",
    "context_string += f\"Top wearing retrieved context {i+1}:\\n\"\n",
    "for (i,doc) in enumerate(raw_context_top.get('result').get('data_array')):\n",
    "    context_string += f\"- name={doc[0]}, image={doc[2]}, description={doc[1]};\"\n",
    "    context_string += \"\\n\\n\"\n",
    "\n",
    "context_string += f\"Bottom wearing retrieved context {i+1}:\\n\"\n",
    "for (i,doc) in enumerate(raw_context_bottom.get('result').get('data_array')):\n",
    "    context_string +=f\"- name={doc[0]}, image={doc[2]}, description={doc[1]};\"\n",
    "    context_string += \"\\n\\n\"\n",
    "\n",
    "context_string += f\"Shoes wearing retrieved context {i+1}:\\n\"\n",
    "for (i,doc) in enumerate(raw_context_shoes.get('result').get('data_array')):\n",
    "    context_string += f\"- name={doc[0]}, image={doc[2]}, description={doc[1]};\"\n",
    "    context_string += \"\\n\\n\"\n",
    "\n",
    "chat.reply(f\"User question: {user_question}\\n\\nContext: {context_string}.\\n\\n Provide an image url for all products described. Be short but descriptive and emphatic\")\n",
    "\n",
    "print(f\"User question: {user_question}\\n\\nDressMe answer: {chat.last}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "dressme-luxury-fashion-rag",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
